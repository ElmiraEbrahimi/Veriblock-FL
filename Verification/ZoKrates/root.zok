import "utils/casts/field_to_u32" as field_to_u32;
import "utils/casts/u32_to_field" as u32_to_field;
import "utils/casts/field_to_u64" as field_to_u64;
import "utils/casts/u64_to_field" as u64_to_field;

const field max = 0-1;
const field threshold = 20888242871839275222246405745257275088548364400416034343698204186575808495617;
const u32  fe = 9;
const u32  ac = 6;
const field ac_f=6;
const u32  bs = 10;

def add(field mut a, field mut b, field mut a_sign, field mut b_sign) -> (field, field) {
    field mut a_cleansed = if a_sign==0 { a } else { max-a+1};
    field mut b_cleansed = if b_sign==0  {b} else {max-b+1};
    field mut c = if a_sign == b_sign && a_sign == 1 {(max+1-a_cleansed-b_cleansed)} else {a+b};
    field mut c_sign = if c > threshold {1} else {0} ;
    return (c, c_sign);
}


def subtract(field mut a, field mut b, field mut a_sign, field mut b_sign) -> (field, field) {
    field mut a_cleansed = if a_sign==0 {a} else {max-a+1} ;
    field mut b_cleansed = if b_sign==0 {b} else {max-b+1} ;
    field mut c = if a_sign != b_sign && a_sign==0 {a_cleansed+b_cleansed} else {a-b} ;
    field mut d = if c > threshold {1} else {0} ;
    return (c, d);
}


def divide(field mut a, field mut b, field mut a_sign, field mut b_sign) -> (field, field) {
    field mut a_cleansed = if a_sign==0 {a} else {max-a+1};
    field mut b_cleansed = if b_sign==0 {b} else {max-b+1};
    // Convert to u64 for easier arithmetic
    u64 a_u64 =field_to_u64(a_cleansed);
    u64 b_u64 = field_to_u64(b_cleansed);
     // Compute the remainder
    u64 remainder = a_u64%b_u64;
    // Adjust the numerator to ensure it's divisible by the denominator
    a_cleansed = u64_to_field(a_u64-remainder);
    // Determine the result
    field mut res = a_cleansed/b_cleansed;
    // Determine the sign of the result
    field mut sign = if a_sign==b_sign || res==0 {0} else {1};
    res = if sign == 0 {res} else {max+1-res};
    return (res, sign);
}

def multiply(field mut a, field mut b, field mut a_sign, field mut b_sign) -> (field, field) {
    field mut a_cleansed = if a_sign==0 {a} else {max-a+1};
    field mut b_cleansed = if b_sign==0 {b} else {max-b+1};
    field mut res = a_cleansed*b_cleansed;
    field mut sign = if a_sign==b_sign || res==0 {0} else {1};
    res = if sign==0 {res} else {max-res+1};
    return (res, sign);
}

// calculate the derivative of the Mean Squared Error (MSE) loss with respect to the predicted values
// pr: A field element representing a precision value
def mse_prime(field[ac] mut y_true, field[ac] mut y_pred, field[ac] mut y_pred_sign, field mut pr) -> (field[ac], field[ac]) {
    field[ac] mut result = [0; ac];
    field[ac] mut result_sign = [0; ac];
    for u32 i in 0..ac {
        (field, field) mut tres = subtract(y_pred[i],y_true[i],y_pred_sign[i],0);
        field mut temp = tres.0;
        field mut temp_sign = tres.1;
        (field, field) mut tres2 = multiply(temp, 2, temp_sign, 0);
        temp = tres2.0;
        temp_sign = tres2.1;
        (field, field) mut tres3 = divide(temp, ac_f, temp_sign, 0);
        result[i] = tres3.0;
        result_sign[i] = tres3.1;
    }
    return (result, result_sign);
}

// def mse(field[ac] mut y_true, field[ac] mut y_pred, field[ac] mut y_pred_sign, field mut pr) -> (field, field) {
//    field mut sum = 0;
//     field mut sum_sign = 0;
//     for u32 i in 0..ac {
//         // Calculate squared difference for each pair
//         (field, field) mut diff = subtract(y_pred[i], y_true[i], y_pred_sign[i], 0);
//         field mut squared_diff = multiply(diff.0, diff.0, diff.1, diff.1).0;
        
//         // Adjust for precision: precision^2
//         field mut precision_squared = multiply(pr, pr, 0, 0).0;
//         field mut adjusted_diff = divide(squared_diff, precision_squared, 0, 0).0;
        
//         // Sum the adjusted squared differences
//         (sum, sum_sign) = add(sum, adjusted_diff, sum_sign, 0);
//     }
    
//     // Calculate mean by dividing the sum by the count (ac), considering precision
//     field mut mean = divide(sum, ac_f, sum_sign, 0).0;
    
//     // The function returns the mean and its sign
//     return (mean, 0); // Assuming mean sign is always positive for MSE
    
// }
def mimc_cipher(field mut input, field[64] mut round_constants, field mut k) -> field{
  
  field mut a = 0;
  
  for u32 i in 0..64 {
    a = input + round_constants[i] + k;
    input = a ** 7;
  }
   return (input + k);
}

def mimc_hash(field[ac][fe] mut w, field[ac] mut b, field[64] mut round_constants) -> field {
    field mut k = 0;

    // Iterate over the activations/layers
    for u32 i in 0..ac {
        // Iterate over the features/weights per activation
        for u32 j in 0..fe {
            k = mimc_cipher(w[i][j], round_constants, k);
        }
        // Process the bias for each activation/layer
        k = mimc_cipher(b[i], round_constants, k);
    }

    return k;
}

// Forward Propagation Loop/Normalization
def forward_propagation_layer(field[ac][fe] mut w, field[ac] b, field[fe] mut x, field[ac][fe] mut w_sign, field[ac] mut b_sign, field[fe] mut x_sign, field mut pr) -> (field[ac], field[ac]) {
    field[ac] mut result = b;
    field[ac] mut result_sign=b_sign;
    field[ac] mut wx=b;
    field[ac] mut wx_sign=b;
    // Iterate through each neuron j in the layer
    for u32 j in 0..ac {
        field mut temp = 0;
        field mut temp_sign = 0;
        //Calculate the dot product of weights (w) and inputs (x)
        for u32 i in 0..fe {
            (field, field) tres = multiply(w[j][i], x[i], w_sign[j][i], x_sign[i]);
            field mut t_i = tres.0;
            field mut t_i_s = tres.1;
            (field, field) mut tres2 = add(temp, t_i, temp_sign, t_i_s);
            temp = tres2.0;
            temp_sign = tres2.1;
        }
        // Divide the dot product by by the precision (pr) for normalzation
        (field, field) mut tres3 = divide(temp, pr, temp_sign, 0);
        temp = tres3.0;
        temp_sign = tres3.1;
        // Store the result of the dot product in wx[j]
        wx[j] = temp;
        wx_sign[j] = temp_sign;
    }
    // Add the dot product result (wx[i]) to the bias (b[i]) for each neuron i.
    for u32 i in 0..ac {
        (field, field) mut tres4 = add(wx[i], b[i], wx_sign[i], b_sign[i]);
        result[i] = tres4.0;
        result_sign[i] = tres4.1;
    }
    return (result, result_sign);
}

def backward_propagation_layer(field[ac][fe] mut w, field[ac] mut b, field[fe] mut x, field[ac] mut output_error, field mut learning_rate, field mut pr, field[ac][fe] mut w_sign,field[ac] mut b_sign,field[fe] mut x_sign,field[ac] mut output_error_sign) -> (field[ac][fe], field[ac], field[ac][fe], field[ac]) {
    // Update Biases (b)
    for u32 i in 0..ac {
        //tres shouldn't have mut ??
        (field, field) tres = divide(output_error[i], learning_rate, output_error_sign[i], 0);
        field temp = tres.0;
        field temp_sign = tres.1;
        (field, field) mut tres2 = subtract(b[i], temp, b_sign[i], temp_sign);
        b[i] = tres2.0;
        b_sign[i] = tres2.1;
    }
    // Update Weights (w). a nested loop over input features j and output neurons i.
    for u32 j in 0..fe {
        for u32 i in 0..ac {
            (field, field) tres = multiply(output_error[i], x[j], output_error_sign[i], x_sign[j]);
            field mut temp = tres.0;
            field mut temp_sign = tres.1;
            (field, field) tres2 = divide(temp, learning_rate, temp_sign, 0);
            temp = tres2.0;
            temp_sign = tres2.1;
            (field, field) tres3 = divide(temp,pr,temp_sign,0);
            temp = tres3.0;
            temp_sign = tres3.1;
            (field, field) tres4 = subtract(w[i][j], temp, w_sign[i][j], temp_sign);
            w[i][j] = tres4.0;
            w_sign[i][j] = tres4.1;
        }
    }
    return (w, b, w_sign, b_sign);
}

//,field[bs][fe] x_train, field[bs][fe] x_train_sign ,field[bs] y_train,field learning_rate,field precision
def main(private field[ac][fe] mut w, private field[ac][fe] mut w_sign,
    private field[ac] mut b, private field[ac] mut b_sign, 
    private field[bs][fe] mut x_train, private field[bs][fe] mut x_train_sign,private field[bs] mut y_train,
    field mut learning_rate,field mut pr,
    private field[ac][fe] mut w_new, private field[ac] mut b_new, public field mut digest,
    public field mut sc_global_params_hash) -> bool {
    
    // output values from the forward propagation
    field[ac] mut output_layer=[0;ac];
    field[ac] mut out_sign=[0;ac];
    // error values calculated during training
    field[ac] mut error = [0;ac];
    field[ac] mut error_sign = [0;ac];
    //intermediate values during the backward propagation
    field[ac] mut backward_layer = [0;ac];
    // store intermediate values during the forward propagation
    field[fe] mut out =[0;fe];
    field[ac] mut backward_layer_sign = [0;ac];
    // store the true labels for the current batch during training
    field[ac] mut y_true = [0;ac];
    // loop iterating over each batch in the training set
    for u32 batch_idx in 0..bs {
        //Extract the input sample (x_train[batch_idx]) for the current batch
        field[fe] mut sample = x_train[batch_idx];
        field[fe] mut sample_sign = x_train_sign[batch_idx];
        // -1: To make the label suitable for indexing arrays or lists in a zero-indexed language
        field mut idx1 = y_train[batch_idx] - 1;
        for u32 i in 0..ac {
            field mut idx2=u32_to_field(i);
            //If they are equal, it means the current class is the correct class for the given batch.
            y_true[i] = if idx1==idx2 {pr} else {0};
        }
 // calculate the output layer:
        (field[ac], field[ac]) tres = forward_propagation_layer(w, b, sample, w_sign, b_sign, sample_sign, pr);
        output_layer = tres.0;
        out_sign = tres.1;
        (field[ac], field[ac]) tres2 = mse_prime(y_true, output_layer, out_sign, pr);
        error = tres2.0;
        error_sign = tres2.1;
        (field[ac][fe], field[ac], field[ac][fe], field[ac]) tres3 = backward_propagation_layer(w, b, sample,error, learning_rate, pr, w_sign, b_sign, sample_sign, error_sign);
        w = tres3.0;
        b = tres3.1;
        w_sign = tres3.2;
        b_sign = tres3.3;
    }

    // validate the results:
    field mut res = 0;
    for u32 i in 0..ac {
        res = res + (if b[i] == b_new[i] {1} else {0});
    }
     for u32 j in 0..fe {
        for u32 i in 0..ac {
            res = res + (if w[i][j] == w_new[i][j] {1} else {0});
        }
    }

    assert(res == u32_to_field(ac + fe * ac));

    // MIMC hash constant 
    field[64] round_constants = [
        42, 43, 170, 2209, 16426, 78087, 279978, 823517, 2097194, 4782931,
        10000042, 19487209, 35831850, 62748495, 105413546, 170859333,
        268435498, 410338651, 612220074, 893871697, 1280000042, 1801088567,
        2494357930, 3404825421, 4586471466, 6103515587, 8031810218, 10460353177,
        13492928554, 17249876351, 21870000042, 27512614133, 34359738410,
        42618442955, 52523350186, 64339296833, 78364164138, 94931877159,
        114415582634, 137231006717, 163840000042, 194754273907, 230539333290,
        271818611081, 319277809706, 373669453167, 435817657258, 506623120485,
        587068342314, 678223072891, 781250000042, 897410677873, 1028071702570,
        1174711139799, 1338925210026, 1522435234413, 1727094849578,
        1954897493219, 2207984167594, 2488651484857, 2799360000042,
        3142742835999, 3521614606250, 3938980639125
    ];

    field mut gobal_hash_check =  mimc_hash(w, b, round_constants);
   
    field mut is_current_hash_found = if sc_global_params_hash == gobal_hash_check { 1 } else {0};


    field mut result = if mimc_hash(w_new, b_new, round_constants) == digest {1} else {0};
    return result == 1 && is_current_hash_found ==1;


}

//  The function main takes several parameters:
// w and w_sign are the weights and their signs for the perceptron.
// b and b_sign are the bias and its sign.
// x_train and x_train_sign are the training data and their signs.
// y_train are the training labels.
// learning_rate is the learning rate for the training.
// pr is a parameter related to the activation function.
// w_new and b_new are the updated weights and bias.
